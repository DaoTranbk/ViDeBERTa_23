
  0%|                                                                                                                                                                                            | 0/2853 [00:00<?, ?it/s]/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '






































































































































































 17%|███████████████████████████████▏                                                                                                                                                  | 499/2853 [05:42<29:54,  1.31it/s]
 18%|███████████████████████████████▏                                                                                                                                                  | 500/2853 [05:43<32:09,  1.22it/s][INFO|trainer.py:2510] 2022-10-06 15:23:35,655 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-500
[INFO|configuration_utils.py:451] 2022-10-06 15:23:35,656 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-500/config.json
[INFO|modeling_utils.py:1550] 2022-10-06 15:23:37,153 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2143] 2022-10-06 15:23:37,155 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2150] 2022-10-06 15:23:37,155 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 15:23:37,198 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-500/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '






































































































































































 35%|██████████████████████████████████████████████████████████████                                                                                                                   | 1000/2853 [11:23<28:40,  1.08it/s][INFO|trainer.py:2510] 2022-10-06 15:29:16,028 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-1000
[INFO|configuration_utils.py:451] 2022-10-06 15:29:16,029 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-1000/config.json
{'loss': 0.0595, 'learning_rate': 3.247458815282159e-05, 'epoch': 1.05}
[INFO|modeling_utils.py:1550] 2022-10-06 15:29:17,571 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2143] 2022-10-06 15:29:17,572 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2150] 2022-10-06 15:29:17,573 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-1000/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 15:29:17,627 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-1000/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '









































































































































































 53%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                    | 1499/2853 [17:07<14:05,  1.60it/s]
 53%|█████████████████████████████████████████████████████████████████████████████████████████████                                                                                    | 1500/2853 [17:09<18:52,  1.20it/s][INFO|trainer.py:2510] 2022-10-06 15:35:01,689 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-1500
[INFO|configuration_utils.py:451] 2022-10-06 15:35:01,691 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-1500/config.json
[INFO|modeling_utils.py:1550] 2022-10-06 15:35:03,474 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2143] 2022-10-06 15:35:03,475 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2150] 2022-10-06 15:35:03,476 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-1500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 15:35:03,516 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-1500/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '








































































































































































 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                     | 2000/2853 [22:54<11:11,  1.27it/s][INFO|trainer.py:2510] 2022-10-06 15:40:47,607 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-2000
[INFO|configuration_utils.py:451] 2022-10-06 15:40:47,609 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-2000/config.json
{'loss': 0.0364, 'learning_rate': 1.4949176305643184e-05, 'epoch': 2.1}
[INFO|modeling_utils.py:1550] 2022-10-06 15:40:49,291 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-2000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2143] 2022-10-06 15:40:49,292 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2150] 2022-10-06 15:40:49,293 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-2000/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 15:40:49,344 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-2000/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '







































































































































































 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                      | 2500/2853 [28:36<04:18,  1.36it/s][INFO|trainer.py:2510] 2022-10-06 15:46:29,185 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-2500
[INFO|configuration_utils.py:451] 2022-10-06 15:46:29,187 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-2500/config.json
{'loss': 0.0298, 'learning_rate': 6.186470382053978e-06, 'epoch': 2.63}
[INFO|modeling_utils.py:1550] 2022-10-06 15:46:30,917 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-2500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2143] 2022-10-06 15:46:30,919 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2150] 2022-10-06 15:46:30,920 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-2500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 15:46:30,973 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-2500/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
























































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2853/2853 [32:46<00:00,  1.78it/s][INFO|trainer.py:1766] 2022-10-06 15:50:39,230 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
{'train_runtime': 1973.6721, 'train_samples_per_second': 23.106, 'train_steps_per_second': 1.446, 'train_loss': 0.0825016290482161, 'epoch': 3.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2853/2853 [32:46<00:00,  1.45it/s]
[INFO|trainer.py:2510] 2022-10-06 15:50:39,246 >> Saving model checkpoint to ../tmp/pos_vlsp2016
[INFO|configuration_utils.py:451] 2022-10-06 15:50:39,248 >> Configuration saved in ../tmp/pos_vlsp2016/config.json
[INFO|modeling_utils.py:1550] 2022-10-06 15:50:41,167 >> Model weights saved in ../tmp/pos_vlsp2016/pytorch_model.bin
[INFO|tokenization_utils_base.py:2143] 2022-10-06 15:50:41,169 >> tokenizer config file saved in ../tmp/pos_vlsp2016/tokenizer_config.json
[INFO|tokenization_utils_base.py:2150] 2022-10-06 15:50:41,170 >> Special tokens file saved in ../tmp/pos_vlsp2016/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 15:50:41,230 >> Copy vocab file to ../tmp/pos_vlsp2016/spiece.model
[INFO|trainer.py:2760] 2022-10-06 15:50:41,286 >> ***** Running Evaluation *****
[INFO|trainer.py:2762] 2022-10-06 15:50:41,287 >>   Num examples = 2045
[INFO|trainer.py:2765] 2022-10-06 15:50:41,287 >>   Batch size = 16
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.0825
  train_runtime            = 0:32:53.67
  train_samples            =      15201
  train_samples_per_second =     23.106
  train_steps_per_second   =      1.446
10/06/2022 15:50:41 - INFO - __main__ - *** Evaluate ***































































































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [11:31<00:00,  5.19s/it]
***** eval metrics *****
  epoch                   =        3.0
  eval_gen_len            =   119.7516
  eval_loss               =      0.039
  eval_rouge1             =    97.1173
  eval_rouge2             =    93.2729
  eval_rougeL             =    96.6264
  eval_rougeLsum          =    96.6338
  eval_runtime            = 0:11:50.32
  eval_samples            =       2045
  eval_samples_per_second =      2.879

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [11:45<00:00,  5.51s/it]
[INFO|modelcard.py:460] 2022-10-06 16:02:32,685 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 97.1173}]}