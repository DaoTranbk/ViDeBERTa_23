
  0%|                                                                                                                                                                          | 0/2853 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-10-06 16:27:36,416 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '















































































































































































 18%|████████████████████████████                                                                                                                                    | 500/2853 [06:04<27:14,  1.44it/s][INFO|trainer.py:2669] 2022-10-06 16:33:40,786 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-500
[INFO|configuration_utils.py:442] 2022-10-06 16:33:40,788 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-500/config.json
{'loss': 0.283, 'learning_rate': 4.12372940764108e-05, 'epoch': 0.53}
[INFO|modeling_utils.py:1583] 2022-10-06 16:33:48,413 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-10-06 16:33:48,419 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-10-06 16:33:48,420 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 16:33:48,466 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-500/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '






































































































































































 35%|███████████████████████████████████████████████████████▋                                                                                                       | 1000/2853 [12:02<24:10,  1.28it/s][INFO|trainer.py:2669] 2022-10-06 16:39:39,137 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-1000
[INFO|configuration_utils.py:442] 2022-10-06 16:39:39,138 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-1000/config.json
{'loss': 0.0595, 'learning_rate': 3.247458815282159e-05, 'epoch': 1.05}
[INFO|modeling_utils.py:1583] 2022-10-06 16:39:45,923 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-10-06 16:39:45,926 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-10-06 16:39:45,927 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-1000/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 16:39:45,983 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-1000/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '






































































































































































 53%|███████████████████████████████████████████████████████████████████████████████████▌                                                                           | 1499/2853 [17:59<14:01,  1.61it/s]
 53%|███████████████████████████████████████████████████████████████████████████████████▌                                                                           | 1500/2853 [18:00<16:27,  1.37it/s][INFO|trainer.py:2669] 2022-10-06 16:45:37,238 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-1500
[INFO|configuration_utils.py:442] 2022-10-06 16:45:37,239 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-1500/config.json
[INFO|modeling_utils.py:1583] 2022-10-06 16:45:44,434 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-10-06 16:45:44,436 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-10-06 16:45:44,437 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-1500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 16:45:44,484 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-1500/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '




















































































































































































 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 2000/2853 [24:28<12:56,  1.10it/s][INFO|trainer.py:2669] 2022-10-06 16:52:05,271 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-2000
[INFO|configuration_utils.py:442] 2022-10-06 16:52:05,272 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-2000/config.json
{'loss': 0.0364, 'learning_rate': 1.4949176305643184e-05, 'epoch': 2.1}
[INFO|modeling_utils.py:1583] 2022-10-06 16:52:13,169 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-2000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-10-06 16:52:13,172 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-10-06 16:52:13,173 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-2000/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 16:52:13,232 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-2000/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '




















































































































































































 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 2498/2853 [30:55<04:13,  1.40it/s]
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 2500/2853 [30:57<05:24,  1.09it/s][INFO|trainer.py:2669] 2022-10-06 16:58:34,126 >> Saving model checkpoint to ../tmp/pos_vlsp2016/checkpoint-2500
[INFO|configuration_utils.py:442] 2022-10-06 16:58:34,127 >> Configuration saved in ../tmp/pos_vlsp2016/checkpoint-2500/config.json
[INFO|modeling_utils.py:1583] 2022-10-06 16:58:42,186 >> Model weights saved in ../tmp/pos_vlsp2016/checkpoint-2500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-10-06 16:58:42,192 >> tokenizer config file saved in ../tmp/pos_vlsp2016/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-10-06 16:58:42,193 >> Special tokens file saved in ../tmp/pos_vlsp2016/checkpoint-2500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 16:58:42,262 >> Copy vocab file to ../tmp/pos_vlsp2016/checkpoint-2500/spiece.model
/media/data/huypn10/linear-transformer/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '































































































































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2851/2853 [35:38<00:01,  1.40it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2853/2853 [35:39<00:00,  1.86it/s][INFO|trainer.py:1873] 2022-10-06 17:03:15,675 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2853/2853 [35:39<00:00,  1.33it/s]
[INFO|trainer.py:2669] 2022-10-06 17:03:15,686 >> Saving model checkpoint to ../tmp/pos_vlsp2016
[INFO|configuration_utils.py:442] 2022-10-06 17:03:15,688 >> Configuration saved in ../tmp/pos_vlsp2016/config.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.0825
  train_runtime            = 0:35:46.14
  train_samples            =      15201
  train_samples_per_second =     21.249
  train_steps_per_second   =      1.329
10/06/2022 17:03:23 - INFO - __main__ - *** Evaluate ***
[INFO|modeling_utils.py:1583] 2022-10-06 17:03:23,571 >> Model weights saved in ../tmp/pos_vlsp2016/pytorch_model.bin
[INFO|tokenization_utils_base.py:2123] 2022-10-06 17:03:23,574 >> tokenizer config file saved in ../tmp/pos_vlsp2016/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2022-10-06 17:03:23,574 >> Special tokens file saved in ../tmp/pos_vlsp2016/special_tokens_map.json
[INFO|tokenization_t5_fast.py:187] 2022-10-06 17:03:23,630 >> Copy vocab file to ../tmp/pos_vlsp2016/spiece.model
[INFO|trainer.py:2920] 2022-10-06 17:03:23,680 >> ***** Running Evaluation *****
[INFO|trainer.py:2922] 2022-10-06 17:03:23,680 >>   Num examples = 2045
[INFO|trainer.py:2925] 2022-10-06 17:03:23,680 >>   Batch size = 16































































































































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [11:04<00:00,  5.67s/it]
***** eval metrics *****
  epoch                   =        3.0
  eval_gen_len            =   119.7516
  eval_loss               =      0.039
  eval_rouge1             =    97.1173
  eval_rouge2             =    93.2729
  eval_rougeL             =    96.6264
  eval_rougeLsum          =    96.6338
  eval_runtime            = 0:11:25.61
  eval_samples            =       2045
  eval_samples_per_second =      2.983

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [11:20<00:00,  5.31s/it]
[INFO|modelcard.py:443] 2022-10-06 17:14:50,425 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 97.1173}]}